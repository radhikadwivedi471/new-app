# -*- coding: utf-8 -*-
"""final engagement project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CrxkLhbjiwFFkb-f3-ec5Ms7wF-Jp4US

#  Multimodal Engagement Rate Predictor (Colab Notebook)

**Goal:** Predict YouTube video Engagement Rate using **text (title, description, tags)**, **tabular** (views, likes, dislikes, comments, categoryId), and **image** (thumbnail)

**Engagement Rate**:  
[
{Engagement Rate} = {likes} + {comment_count}%{view_count}
]

**What this notebook does:**  
1. Installs dependencies and sets up environment (GPU-friendly, T4 tested).  
2. Loads dataset from Google Drive path: `/content/drive/MyDrive/youtube_10k_sample.csv`.  
3. Preprocesses data: cleans, builds text sequences, normalizes tabular, downloads thumbnails, and extracts **CLIP** image embeddings.  
5. Trains multiple models: **Text-only**, **Tabular-only**, **Image-only**, **Early Fusion**, **Late Fusion**, **Hybrid Fusion**.  
6. Evaluates with **MAE, MSE,RMSE,R2 ** (lower is better),  compares with **bar charts**, comapre them together also and provides **explainability**:  
   - **SHAP** for tabular features  
   - **Attention heatmaps** for text (token attention) and **CLIP-ViT attention rollout** heatmaps for images  
7. Provides a **prediction cell**: paste a YouTube URL, it fetches metadata + thumbnail and outputs predicted engagement rate.

> **Note:** This notebook is designed to **run end-to-end** in Google Colab on a T4 GPU.

##  1) Setup & Installs
"""

# ========================================
# Google Colab Setup (Default Python 3.10)
# ========================================

# ---- Upgrade pip ----
!pip install -U pip

# ---- Core PyTorch + CUDA 12.1 ----
!pip install -q torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121


# ---- OpenCLIP + HuggingFace ----
!pip install -q open-clip-torch transformers==4.44.2 sentencepiece==0.2.0

# ---- Other utilities ----
!pip install -q pytube pillow scikit-learn pandas numpy tqdm shap umap-learn pyyaml

# ========================================
# Verification
# ========================================
import sys, torch, torchvision, torchaudio, sklearn, pandas as pd, numpy as np
import transformers, open_clip, pytube, PIL, tqdm, shap, umap, yaml

print(" Python version:", sys.version)
print(" Torch:", torch.__version__)
print(" Torchvision:", torchvision.__version__)
print(" Torchaudio:", torchaudio.__version__)
print(" Torch CUDA available:", torch.cuda.is_available())
print(" Transformers:", transformers.__version__)
print(" OpenCLIP:", open_clip.__version__)

# ====== Core Python ======
import os
import sys
import random

# ====== Data Handling ======
import numpy as np
import pandas as pd

# ====== Machine Learning / Deep Learning ======
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision
from torchvision import transforms

# ====== HuggingFace / NLP ======
from transformers import AutoTokenizer, AutoModel, CLIPProcessor, CLIPModel

# ====== OpenCLIP ======
import open_clip

# ====== Metrics ======
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ====== Visualization ======
import matplotlib.pyplot as plt
import seaborn as sns

# ====== Utilities ======
from tqdm import tqdm
import yaml
import shap
import umap
import re
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

"""##  2) Config & Drive Mount"""

from google.colab import drive
drive.mount('/content/drive')

# === Config ===
DATA_PATH = "/content/drive/MyDrive/youtube_10k_sample.csv"  # <-- required
WORK_DIR = "/content/youtube_engagement_multimodal_gnn"
IMG_DIR = f"{WORK_DIR}/thumbnails"
EMB_DIR = f"{WORK_DIR}/embeddings"
MODEL_DIR = f"{WORK_DIR}/models"
os.makedirs(WORK_DIR, exist_ok=True)
os.makedirs(IMG_DIR, exist_ok=True)
os.makedirs(EMB_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

SEED = 42
np.random.seed(SEED)
import torch
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# Sanity check for data path
import os
assert os.path.exists(DATA_PATH), f"CSV not found at {DATA_PATH} â€” please upload to Drive at that exact path."
print("Dataset found ")

"""##  3) Load Data & Basic Cleaning"""

import pandas as pd
import numpy as np

df = pd.read_csv(DATA_PATH)
print("Raw shape:", df.shape)
print(df.head(3))

# Keep essential columns, fill/clean basic issues
required_cols = [
    "video_id","title","publishedAt","channelId","channelTitle","categoryId",
    "trending_date","tags","view_count","likes","dislikes",
    "comment_count","thumbnail_link","description"
]
missing = [c for c in required_cols if c not in df.columns]
assert not missing, f"Missing required columns: {missing}"

# Drop obvious invalid rows (missing critical fields or zero views when likes/comments present might be okay â€” we handle via smoothing)
df = df.dropna(subset=["video_id","title","publishedAt","channelId","categoryId","trending_date","view_count","thumbnail_link"]).copy()

# Fix data types
df["view_count"] = pd.to_numeric(df["view_count"], errors="coerce").fillna(0).clip(lower=0)
df["likes"] = pd.to_numeric(df["likes"], errors="coerce").fillna(0).clip(lower=0)
df["dislikes"] = pd.to_numeric(df["dislikes"], errors="coerce").fillna(0).clip(lower=0)
df["comment_count"] = pd.to_numeric(df["comment_count"], errors="coerce").fillna(0).clip(lower=0)
df["categoryId"] = pd.to_numeric(df["categoryId"], errors="coerce").fillna(-1).astype(int)

# Datetime parsing
df["publishedAt"] = pd.to_datetime(df["publishedAt"], errors="coerce", utc=True)
df["trending_date"] = pd.to_datetime(df["trending_date"], errors="coerce", utc=True)

# Simple date-time features
df["publish_hour"] = df["publishedAt"].dt.hour.fillna(0).astype(int)
df["publish_dayofweek"] = df["publishedAt"].dt.dayofweek.fillna(0).astype(int)
df["time_to_trend_hours"] = ((df["trending_date"] - df["publishedAt"]).dt.total_seconds()/3600.0).replace([np.inf,-np.inf], np.nan).fillna(0)

# Target: Engagement Rate
df["engagement_rate"] = (df["likes"] + df["comment_count"]) / (df["view_count"] + 1e-6)

# Clean tags (they can be '|' separated or comma separated; handle both)
def clean_tags(t):
    if pd.isna(t): return []
    s = str(t)
    if "|" in s:
        parts = [p.strip().lower() for p in s.split("|") if p.strip()]
    else:
        parts = [p.strip().lower() for p in s.split(",") if p.strip()]
    return parts[:20]  # cap length
df["tags_list"] = df["tags"].apply(clean_tags)

# Basic text fill
for col in ["title","description","channelTitle"]:
    df[col] = df[col].fillna("").astype(str)

# Keep a working subset if needed (optional). Uncomment to speed up prototyping.
# df = df.sample(4000, random_state=SEED).reset_index(drop=True)

print("Cleaned shape:", df.shape)
df.head(2)

"""##  4) Text Preprocessing (Tokenizer + Sequences)"""

from collections import Counter

def basic_tokenize(s):
    s = s.lower()
    s = re.sub(r"http\S+"," ", s)
    s = re.sub(r"[^a-z0-9#@_\-\s]", " ", s)
    s = re.sub(r"\s+"," ", s).strip()
    return s.split()

# Build a single text field combining title + description + tags (with caps to keep length reasonable)
MAX_TITLE = 32
MAX_DESC = 128
def build_text(row):
    title = " ".join(basic_tokenize(row["title"]))[:MAX_TITLE*2]
    desc = " ".join(basic_tokenize(row["description"]))[:MAX_DESC*2]
    tags = " ".join(row["tags_list"])
    return f"{title} [SEP] {desc} [SEP] {tags}"

df["text_all"] = df.apply(build_text, axis=1)

# Vocab
all_tokens = []
for s in df["text_all"].tolist():
    all_tokens.extend(s.split())

min_freq = 3
counts = Counter(all_tokens)
itos = ["<pad>","<unk>"] + [w for w,c in counts.items() if c >= min_freq]
stoi = {w:i for i,w in enumerate(itos)}

def encode(text, max_len=160):
    toks = text.split()
    ids = [stoi.get(t, stoi["<unk>"]) for t in toks][:max_len]
    attn = [1]*len(ids)
    if len(ids) < max_len:
        pad = max_len - len(ids)
        ids = ids + [stoi["<pad>"]]*pad
        attn = attn + [0]*pad
    return np.array(ids, dtype=np.int64), np.array(attn, dtype=np.int64)

ids_list, attn_list = [], []
for s in df["text_all"].tolist():
    ids, attn = encode(s, max_len=160)
    ids_list.append(ids); attn_list.append(attn)

import numpy as np
text_ids = np.stack(ids_list)
text_attn = np.stack(attn_list)

vocab_size = len(itos)
print("Vocab size:", vocab_size, "Text tensor shape:", text_ids.shape)

"""##  5) Tabular Preprocessing"""

from sklearn.preprocessing import StandardScaler

tab_cols = ["view_count","likes","dislikes","comment_count","categoryId",
            "publish_hour","publish_dayofweek","time_to_trend_hours"]
tab = df[tab_cols].fillna(0).values.astype(np.float32)

scaler = StandardScaler()
tab_scaled = scaler.fit_transform(tab).astype(np.float32)

print("Tabular shape:", tab_scaled.shape)

"""##  6) Thumbnail Download + CLIP Image Embeddings"""

import os, requests
from PIL import Image
from io import BytesIO
from tqdm import tqdm
import torch
import open_clip
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"

# Download thumbnails
def safe_filename(vid):
    return "".join([c if c.isalnum() else "_" for c in str(vid)])

def download_image(url, out_path, timeout=10):
    try:
        r = requests.get(url, timeout=timeout)
        r.raise_for_status()
        img = Image.open(BytesIO(r.content)).convert("RGB")
        img.save(out_path, format="JPEG", quality=90)
        return True
    except Exception as e:
        return False

thumb_paths = []
for vid, url in tqdm(zip(df["video_id"], df["thumbnail_link"]), total=len(df), desc="Downloading thumbnails"):
    fname = f"{IMG_DIR}/{safe_filename(vid)}.jpg"
    if not os.path.exists(fname):
        ok = download_image(str(url), fname)
        if not ok:
            # create a blank placeholder if download fails
            Image.new("RGB", (320, 180), color=(127,127,127)).save(fname, "JPEG", quality=90)
    thumb_paths.append(fname)

df["thumb_path"] = thumb_paths

# Load CLIP (ViT-B-32 for speed)
model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai', device=device)
model.eval()

# Compute image embeddings (batched for speed)
BATCH = 64
image_embeds = []

def image_batch(paths):
    imgs = []
    for p in paths:
        try:
            img = Image.open(p).convert("RGB")
        except:
            img = Image.new("RGB", (320,180), color=(127,127,127))
        imgs.append(preprocess(img))
    return torch.stack(imgs)

with torch.no_grad():
    for i in tqdm(range(0, len(df), BATCH), desc="CLIP embedding"):
        batch_paths = df["thumb_path"].iloc[i:i+BATCH].tolist()
        ims = image_batch(batch_paths).to(device)
        feats = model.encode_image(ims)
        feats = feats / feats.norm(dim=-1, keepdim=True)
        image_embeds.append(feats.detach().cpu().numpy())

image_embeds = np.concatenate(image_embeds, axis=0).astype(np.float32)
print("Image embeddings shape:", image_embeds.shape)

"""**YTdataset**"""

import torch
from torch.utils.data import Dataset, DataLoader

class YTDataset(Dataset):
    def __init__(self, idxs):
        self.idxs = idxs
    def __len__(self):
        return len(self.idxs)
    def __getitem__(self, i):
        j = self.idxs[i]
        return (
            torch.tensor(text_ids[j], dtype=torch.long),
            torch.tensor(text_attn[j], dtype=torch.long),
            torch.tensor(tab_scaled[j], dtype=torch.float32),
            torch.tensor(image_embeds[j], dtype=torch.float32),
            torch.tensor(y[j], dtype=torch.float32),
        )

"""##  7) Train/Val/Test - Split"""

from sklearn.model_selection import train_test_split

y = df["engagement_rate"].values.astype(np.float32)
idx = np.arange(len(df))

train_idx, test_idx = train_test_split(idx, test_size=0.15, random_state=SEED)
train_idx, val_idx = train_test_split(train_idx, test_size=0.1765, random_state=SEED)  # 0.1765*0.85 â‰ˆ 0.15

def take(a, idc):
    return a[idc]

splits = {
    "train": train_idx,
    "val": val_idx,
    "test": test_idx
}

print({k: len(v) for k,v in splits.items()})

"""##  8) Torch Datasets & Loaders"""

from sklearn.model_selection import train_test_split

# Assuming you have a dataframe `df` with all samples
train_idx, temp_idx = train_test_split(np.arange(len(df)), test_size=0.3, random_state=42)
val_idx, test_idx   = train_test_split(temp_idx, test_size=0.5, random_state=42)

# Create dataset splits
train_dataset = YTDataset(train_idx)
val_dataset   = YTDataset(val_idx)
test_dataset  = YTDataset(test_idx)

from torch.utils.data import DataLoader

BATCH_SIZE = 64  # adjust for GPU memory

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

"""##  9) Base Models (Text / Tabular / Image) + Fusion Variants"""

import torch
import torch.nn as nn
import torch.nn.functional as F

EMB_DIM = 128
HID_DIM = 192
IMG_DIM = image_embeds.shape[1]
TAB_DIM = tab_scaled.shape[1]

class AttentionPool(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.w = nn.Linear(dim, 1)
    def forward(self, x, mask):
        # x: (B, T, D), mask: (B, T) 1=valid
        scores = self.w(x).squeeze(-1)  # (B,T)
        scores = scores.masked_fill(mask==0, -1e9)
        attn = torch.softmax(scores, dim=-1).unsqueeze(-1)  # (B,T,1)
        out = (x * attn).sum(dim=1)  # (B,D)
        return out, attn.squeeze(-1)

class TextModel(nn.Module):
    def __init__(self, vocab_size, emb_dim=EMB_DIM, hid_dim=HID_DIM):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
        self.lstm = nn.LSTM(emb_dim, hid_dim//2, num_layers=1, bidirectional=True, batch_first=True)
        self.attn = AttentionPool(hid_dim)
        self.head = nn.Sequential(nn.LayerNorm(hid_dim), nn.Dropout(0.2), nn.Linear(hid_dim, 1))
    def forward(self, ids, mask):
        x = self.emb(ids)
        x,_ = self.lstm(x)
        pooled, attn = self.attn(x, mask)
        out = self.head(pooled).squeeze(-1)
        return out, attn, pooled  # return pooled for fusion

class TabularModel(nn.Module):
    def __init__(self, in_dim=TAB_DIM):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.2),
            nn.Linear(128, 64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.1),
        )
        self.head = nn.Linear(64, 1)
    def forward(self, x):
        h = self.mlp(x)
        out = self.head(h).squeeze(-1)
        return out, h

class ImageModel(nn.Module):
    def __init__(self, in_dim=IMG_DIM):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, 256), nn.ReLU(), nn.BatchNorm1d(256), nn.Dropout(0.2),
            nn.Linear(256, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.1),
        )
        self.head = nn.Linear(128, 1)
    def forward(self, x):
        h = self.mlp(x)
        out = self.head(h).squeeze(-1)
        return out, h

# === Fusion models ===
class EarlyFusion(nn.Module):
    def __init__(self, txt_dim=HID_DIM, tab_dim=64, img_dim=128):
        super().__init__()
        self.text = TextModel(vocab_size)
        self.tab = TabularModel(TAB_DIM)
        self.img = ImageModel(IMG_DIM)
        fused_dim = txt_dim + 64 + 128
        self.fuse = nn.Sequential(
            nn.Linear(fused_dim, 256), nn.ReLU(), nn.Dropout(0.2),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(128, 1)
        )
    def forward(self, ids, mask, tab, img):
        t_out, t_attn, t_feat = self.text(ids, mask)
        tab_out, tab_feat = self.tab(tab)
        img_out, img_feat = self.img(img)
        z = torch.cat([t_feat, tab_feat, img_feat], dim=-1)
        yhat = self.fuse(z).squeeze(-1)
        return yhat, {"t_attn": t_attn, "t_feat": t_feat, "tab_feat": tab_feat, "img_feat": img_feat}

class LateFusion(nn.Module):
    def __init__(self):
        super().__init__()
        self.text = TextModel(vocab_size)
        self.tab = TabularModel(TAB_DIM)
        self.img = ImageModel(IMG_DIM)
        self.fuse = nn.Linear(3, 1)
    def forward(self, ids, mask, tab, img):
        t_out, t_attn, _ = self.text(ids, mask)
        tab_out, _ = self.tab(tab)
        img_out, _ = self.img(img)
        logits = torch.stack([t_out, tab_out, img_out], dim=-1)  # (B,3)
        yhat = self.fuse(logits).squeeze(-1)
        return yhat, {"t_attn": t_attn}

class HybridFusion(nn.Module):
    def __init__(self):
        super().__init__()
        self.text = TextModel(vocab_size)
        self.tab = TabularModel(TAB_DIM)
        self.img = ImageModel(IMG_DIM)
        # first combine text+image early, then late fuse with tab
        self.early = nn.Sequential(nn.Linear(HID_DIM + 128, 128), nn.ReLU(), nn.Dropout(0.1))
        self.late = nn.Linear(2, 1)
    def forward(self, ids, mask, tab, img):
        t_out, t_attn, t_feat = self.text(ids, mask)
        img_out, img_feat = self.img(img)
        early = self.early(torch.cat([t_feat, img_feat], dim=-1))
        early_out = torch.tanh(early.mean(dim=-1, keepdim=False))  # scalar-like
        tab_out, _ = self.tab(tab)
        logits = torch.stack([early_out, tab_out], dim=-1)
        yhat = self.late(logits).squeeze(-1)
        return yhat, {"t_attn": t_attn}

# Standalone heads for single-modality baselines
txt_only = TextModel(vocab_size)
tab_only = TabularModel(TAB_DIM)
img_only = ImageModel(IMG_DIM)
early_fusion = EarlyFusion()
late_fusion = LateFusion()
hybrid_fusion = HybridFusion()

models = {
    "Text": txt_only,
    "Tabular": tab_only,
    "Image": img_only,
    "EarlyFusion": early_fusion,
    "LateFusion": late_fusion,
    "HybridFusion": hybrid_fusion,
}
for name, m in models.items():
    m.to(device)
    print(name, "params:", sum(p.numel() for p in m.parameters() if p.requires_grad))

"""##  10) Training Utilities"""

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ---------- Evaluation Function ----------
@torch.no_grad()
def evaluate_metrics(model, loader, device):
    model.eval()
    preds, targets = [], []
    for ids, mask, tab, img, tgt in loader:
        ids, mask, tab, img, tgt = ids.to(device), mask.to(device), tab.to(device), img.to(device), tgt.to(device)

        # Handle unimodal vs multimodal
        if isinstance(model, (TextModel, TabularModel, ImageModel)):
            if isinstance(model, TextModel):
                pred, _, _ = model(ids, mask)
            elif isinstance(model, TabularModel):
                pred, _ = model(tab)
            else:
                pred, _ = model(img)
        else:
            pred, _ = model(ids, mask, tab, img)

        preds.append(pred.detach().cpu().numpy())
        targets.append(tgt.detach().cpu().numpy())

    preds = np.concatenate(preds).ravel()
    targets = np.concatenate(targets).ravel()

    mae = mean_absolute_error(targets, preds)
    mse = mean_squared_error(targets, preds)
    rmse = np.sqrt(mse)
    r2 = r2_score(targets, preds)

    return {"mae": mae, "mse": mse, "rmse": rmse, "r2": r2}

# ---------- Training Loop ----------
def fit_model(model, name, epochs=10, lr=1e-3, device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)

    history = {"train_loss": [], "val_mae": [], "val_mse": [], "val_rmse": [], "val_r2": []}
    best_val, ckpt_path = float("inf"), None

    for epoch in range(epochs):
        # ---- Training ----
        model.train()
        total_loss = 0.0
        for ids, mask, tab, img, tgt in train_loader:
            ids, mask, tab, img, tgt = ids.to(device), mask.to(device), tab.to(device), img.to(device), tgt.to(device)

            opt.zero_grad(set_to_none=True)

            if isinstance(model, (TextModel, TabularModel, ImageModel)):
                if isinstance(model, TextModel):
                    pred, _, _ = model(ids, mask)
                elif isinstance(model, TabularModel):
                    pred, _ = model(tab)
                else:
                    pred, _ = model(img)
            else:
                pred, _ = model(ids, mask, tab, img)

            loss = F.l1_loss(pred, tgt)  # MAE loss
            loss.backward()
            opt.step()
            total_loss += loss.item() * ids.size(0)

        train_loss = total_loss / len(train_loader.dataset)

        # ---- Validation ----
        val_metrics = evaluate_metrics(model, val_loader, device)

        # ---- Logging ----
        history["train_loss"].append(train_loss)
        for k in val_metrics:
            history[f"val_{k}"].append(val_metrics[k])

        print(f"[{name}] Epoch {epoch+1}/{epochs} | "
              f"Train Loss: {train_loss:.4f} | "
              f"Val MAE: {val_metrics['mae']:.4f} | "
              f"Val RMSE: {val_metrics['rmse']:.4f} | "
              f"Val R2: {val_metrics['r2']:.4f}")

        # ---- Save Best ----
        if val_metrics["mae"] < best_val:
            best_val = val_metrics["mae"]
            ckpt_path = f"{name}_best.pth"
            torch.save(model.state_dict(), ckpt_path)

    # ---- Test Evaluation ----
    test_metrics = evaluate_metrics(model, test_loader, device)

    return best_val, test_metrics, ckpt_path, history

"""Training & Evaluation Pipeline with Early Stopping and Metrics.

##  11) Train Baselines & Fusion Models
"""



# -------------------------------------------------------
# Epoch settings for each model
# -------------------------------------------------------
EPOCHS_BASE = {
    "Text": 10,
    "Image": 10,
    "Tabular": 10,
    "EarlyFusion": 15,
    "LateFusion": 15,
    "HybridFusion": 18,
}

results = {}
histories = {}

# Create directory for checkpoints
import os
os.makedirs("checkpoints", exist_ok=True)

# -------------------------------------------------------
# Training Loop over all models
# -------------------------------------------------------
for name, model in models.items():
    print("="*80)
    print(f"ðŸš€ Training: {name}")
    epochs = EPOCHS_BASE[name] if isinstance(EPOCHS_BASE, dict) else EPOCHS_BASE

    best_val_mae, test_metrics, ckpt_path, history = fit_model(
        model,
        name=name,
        epochs=epochs,
        lr=1e-3,
        device=device  # make sure device is defined earlier
    )

    # Fix checkpoint path into "checkpoints/" folder
    if ckpt_path is not None:
        new_ckpt_path = os.path.join("checkpoints", os.path.basename(ckpt_path))
        os.replace(ckpt_path, new_ckpt_path)
        ckpt_path = new_ckpt_path

    # Save results
    results[name] = {
        "best_val_mae": best_val_mae,
        "test_mae":  test_metrics["mae"],
        "test_mse":  test_metrics["mse"],
        "test_rmse": test_metrics["rmse"],
        "test_r2":   test_metrics["r2"],
        "ckpt": ckpt_path,
    }
    histories[name] = history

    # Print summary
    print(f"\n>>>  {name} Results <<<")
    print(f"Best Val MAE: {best_val_mae:.4f}")
    print(f"Test MAE:  {test_metrics['mae']:.4f}")
    print(f"Test MSE:  {test_metrics['mse']:.4f}")
    print(f"Test RMSE: {test_metrics['rmse']:.4f}")
    print(f"Test RÂ²:   {test_metrics['r2']:.4f}\n")

"""## 14) Compare Models (MAE)"""

print(histories.keys())
print(results.keys())

import matplotlib.pyplot as plt
import numpy as np

def plot_final_comparison(results, metrics=["mae", "mse", "rmse", "r2"]):
    model_names = list(results.keys())
    n_models = len(model_names)
    n_metrics = len(metrics)

    # Extract test metrics into matrix shape: [models, metrics]
    values = np.zeros((n_models, n_metrics))
    for i, model in enumerate(model_names):
        for j, m in enumerate(metrics):
            values[i, j] = results[model][f"test_{m}"]

    # Bar settings
    x = np.arange(n_models)
    width = 0.2
    colors = ["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728"]  # Blue, Orange, Green, Red

    plt.figure(figsize=(12, 6))

    # Plot each metric side by side for each model
    for j, m in enumerate(metrics):
        plt.bar(x + j*width, values[:, j], width, label=m.upper(), color=colors[j])

    # Formatting
    plt.xticks(x + width*(n_metrics-1)/2, model_names, rotation=30)
    plt.ylabel("Metric Value")
    plt.title("Final Test Metrics Comparison Across Models")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ---- Call after training ----
plot_final_comparison(results)

import matplotlib.pyplot as plt
import numpy as np

# ------------------------------------------------------
# 1. Validation Curves (one plot per metric across epochs)
# ------------------------------------------------------
def plot_validation_curves(histories, metrics=["mae", "mse", "rmse", "r2"]):
    plt.figure(figsize=(14,10))
    n_metrics = len(metrics)

    for i, m in enumerate(metrics):
        plt.subplot(2, 2, i+1)
        for model_name, hist in histories.items():
            plt.plot(hist[f"val_{m}"], label=model_name)
        plt.title(f"Validation {m.upper()} per Epoch")
        plt.xlabel("Epoch")
        plt.ylabel(m.upper())
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    plt.show()

# ------------------------------------------------------
# 2. Final Metrics Comparison (Val vs Test per metric)
# ------------------------------------------------------
def plot_val_test_comparison(histories, results, metrics=["mae", "mse", "rmse", "r2"]):
    model_names = list(results.keys())
    x = np.arange(len(model_names))
    width = 0.35

    plt.figure(figsize=(14,10))
    n_metrics = len(metrics)

    for i, m in enumerate(metrics):
        plt.subplot(2, 2, i+1)

        # Validation metric = last epoch value
        val_values = [histories[model][f"val_{m}"][-1] for model in model_names]
        test_values = [results[model][f"test_{m}"] for model in model_names]

        plt.bar(x - width/2, val_values, width, label="Validation", alpha=0.7)
        plt.bar(x + width/2, test_values, width, label="Test", alpha=0.7)

        plt.xticks(x, model_names, rotation=30)
        plt.ylabel(m.upper())
        plt.title(f"{m.upper()} Comparison (Val vs Test)")
        plt.legend()
        plt.grid(True, axis="y", linestyle="--", alpha=0.6)

    plt.tight_layout()
    plt.show()

# -----------------
# Example usage:
# -----------------
plot_validation_curves(histories)
plot_val_test_comparison(histories, results)

import matplotlib.pyplot as plt
import numpy as np

# -----------------------------
# Helpers: robust key retrieval
# -----------------------------
def _metric_key_candidates(metric, kind=None):
    """
    Build a list of candidate keys for a given metric.
    kind: None | 'test' | 'val'
    """
    base = metric.lower()
    aliases = {
        "mae": ["mae", "mean_absolute_error"],
        "mse": ["mse", "mean_squared_error"],
        "rmse": ["rmse", "root_mse", "root_mean_squared_error"],
        "r2": ["r2", "r2_score", "r^2"],
    }
    pref = [""]
    if kind == "test": pref = [f"test_", ""]
    if kind == "val":  pref = [f"val_", ""]
    cand = []
    for p in pref:
        for a in aliases.get(base, [base]):
            cand.extend([f"{p}{a}", f"{p}{a}".upper()])
    return cand

def _get_metric(dct, metric, kind=None, default=None, strict=False, model_name=""):
    """
    Get metric value from dict dct using multiple possible keys.
    If strict=True, raise if not found; else return default and print a warning.
    """
    for k in _metric_key_candidates(metric, kind):
        if k in dct:
            return dct[k]
    if strict:
        raise KeyError(f"Missing metric '{metric}' ({'kind='+kind if kind else ''}) in results for model '{model_name}'. "
                       f"Tried keys: {', '.join(_metric_key_candidates(metric, kind))}")
    else:
        print(f"[WARN] '{metric}' ({'kind='+kind if kind else ''}) not found for model '{model_name}'. "
              f"Available: {list(dct.keys())}")
        return default

# ------------------------------------------------------
# 1) Validation Curves (one plot per metric across epochs)
# histories[name] must contain val_* lists
# ------------------------------------------------------
def plot_validation_curves(histories, metrics=["mae", "mse", "rmse", "r2"]):
    plt.figure(figsize=(14, 10))

    for i, m in enumerate(metrics):
        ax = plt.subplot(2, 2, i+1)
        any_line = False
        for model_name, hist in histories.items():
            # robustly look for val_* key
            val_key_candidates = _metric_key_candidates(m, "val")
            val_key = next((k for k in val_key_candidates if k in hist), None)
            if val_key is None:
                print(f"[WARN] No '{m}' validation history for '{model_name}'. Looked for: {val_key_candidates}")
                continue
            ax.plot(hist[val_key], label=model_name)
            any_line = True

        ax.set_title(f"Validation {m.upper()} per Epoch")
        ax.set_xlabel("Epoch")
        ax.set_ylabel(m.upper())
        if any_line:
            ax.legend()
        ax.grid(True, linestyle="--", alpha=0.4)

    plt.tight_layout()
    plt.show()

# ----------------------------------------------------------------------
# 2) Final Metrics Comparison (Validation-last vs Test for each metric)
# results[name] may have keys like test_mae or just mae; histories[name] has val_*
# ----------------------------------------------------------------------
def plot_val_test_comparison(histories, results, metrics=["mae", "mse", "rmse", "r2"]):
    model_names = list(results.keys())
    x_all = np.arange(len(model_names))
    width = 0.35

    plt.figure(figsize=(14, 10))

    for i, m in enumerate(metrics):
        ax = plt.subplot(2, 2, i+1)

        labels, val_values, test_values = [], [], []
        for idx, model in enumerate(model_names):
            # --- Validation last-epoch value ---
            hist = histories.get(model, {})
            val_key = next((k for k in _metric_key_candidates(m, "val") if k in hist), None)
            val_val = None
            if val_key is not None and len(hist[val_key]) > 0:
                val_val = float(hist[val_key][-1])

            # --- Test value ---
            test_val = _get_metric(results[model], m, kind="test", default=None, strict=False, model_name=model)
            if test_val is None and m in results[model]:
                test_val = results[model][m]  # extra fallback

            # Only plot if at least one exists; keep x labels aligned per-metric
            if val_val is not None or test_val is not None:
                labels.append(model)
                val_values.append(val_val if val_val is not None else np.nan)
                test_values.append(test_val if test_val is not None else np.nan)

        x = np.arange(len(labels))
        b1 = ax.bar(x - width/2, val_values, width, label="Validation (last)", alpha=0.8)
        b2 = ax.bar(x + width/2, test_values, width, label="Test", alpha=0.8)

        # Value labels
        for bars in (b1, b2):
            for bar in bars:
                h = bar.get_height()
                if np.isfinite(h):
                    ax.text(bar.get_x() + bar.get_width()/2, h, f"{h:.3f}",
                            ha="center", va="bottom", fontsize=8, rotation=45)

        ax.set_xticks(x)
        ax.set_xticklabels(labels, rotation=30)
        ax.set_ylabel(m.upper())
        ax.set_title(f"{m.upper()} Comparison (Validation vs Test)")
        ax.grid(True, axis="y", linestyle="--", alpha=0.6)
        ax.legend()

    plt.tight_layout()
    plt.show()

# ----------------------------------------------------------------------
# 3) Final Test Metrics Only â€” Grouped bars (robust to key names)
# ----------------------------------------------------------------------
def plot_final_comparison(results, metrics=["mae", "mse", "rmse", "r2"]):
    """
    Grouped bar chart: rows = models, grouped bars = metrics (test values).
    Works whether results store 'test_mae' or just 'mae', etc.
    """
    model_names = list(results.keys())
    n_models = len(model_names)
    n_metrics = len(metrics)

    # Collect values; skip missing values per-model gracefully
    values = np.full((n_models, n_metrics), np.nan)
    for i, model in enumerate(model_names):
        for j, m in enumerate(metrics):
            val = _get_metric(results[model], m, kind="test", default=None, strict=False, model_name=model)
            if val is None and m in results[model]:
                val = results[model][m]
            values[i, j] = np.nan if val is None else float(val)

    x = np.arange(n_models)
    total_width = 0.8
    width = total_width / max(n_metrics, 1)
    colors = ["#4c72b0", "#dd8452", "#55a868", "#c44e52"]

    plt.figure(figsize=(14, 7))
    for j, m in enumerate(metrics):
        bars = plt.bar(x + j*width, values[:, j], width,
                       label=m.upper(),
                       color=colors[j % len(colors)],
                       alpha=0.90)
        # labels
        for bar in bars:
            h = bar.get_height()
            if np.isfinite(h):
                plt.text(bar.get_x() + bar.get_width()/2, h, f"{h:.3f}",
                         ha="center", va="bottom", fontsize=8, rotation=45)

    plt.xticks(x + total_width/2 - width/2, model_names, rotation=30, fontsize=11)
    plt.ylabel("Metric Value", fontsize=12)
    plt.title(" Final Test Metrics Comparison Across Models", fontsize=14, weight="bold")
    plt.grid(axis="y", linestyle="--", alpha=0.5)
    plt.legend(title="Metrics", fontsize=10)
    plt.tight_layout()
    plt.show()



plot_validation_curves(histories)                 # epoch-wise validation curves
    # Val(last) vs Test per metric
plot_final_comparison(results)                    # grouped bars of Test metrics

"""**conclusion**


using all modalities together clearly beats using a single modality.

Among fusions, HybridFusion is the most accurate (lowest MAE).

If you only had to pick one model for deployment â†’ HybridFusion

interpretability

explainability
"""

# ===  Interpretability & Explainability Suite (drop-in cell) ===
# Works with: TextModel (LSTM+Attention), TabularModel (MLP), ImageModel (MLP on image_embeds),
# and fusion models EarlyFusion / LateFusion / HybridFusion defined in this notebook.
# No dataset rework needed.

import math, types, warnings, numpy as np, pandas as pd, torch, torch.nn.functional as F
import matplotlib.pyplot as plt

# Optional Captum for better attributions (will fallback gracefully if missing)
try:
    from captum.attr import IntegratedGradients, Saliency
    _HAS_CAPTUM = True
except Exception as _e:
    warnings.warn(f"Captum not available ({_e}); falling back to gradient*input where possible.")
    _HAS_CAPTUM = False

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Utilities to fetch a single example (by absolute row index in df or split index) ---
def get_example(idx: int):
    """Return a single sample (ids, attn_mask, tab, img, y_true) on DEVICE, plus raw numpy row index."""
    ids = torch.tensor(text_ids[idx], dtype=torch.long, device=DEVICE).unsqueeze(0)
    mask = torch.tensor(text_attn[idx], dtype=torch.long, device=DEVICE).unsqueeze(0)
    tabv = torch.tensor(tab_scaled[idx], dtype=torch.float32, device=DEVICE).unsqueeze(0)
    imgv = torch.tensor(image_embeds[idx], dtype=torch.float32, device=DEVICE).unsqueeze(0)
    yv   = torch.tensor(y[idx], dtype=torch.float32, device=DEVICE).unsqueeze(0)
    return ids, mask, tabv, imgv, yv

def _ensure_eval(model):
    was_train = model.training
    model.eval()
    return was_train

def _restore_mode(model, was_train):
    if was_train: model.train()

# --- 1) Text attention visualization (word importances) ---
def visualize_text_attention(model_text_or_fusion, row_idx: int, top_k=20, show_plot=True):
    """
    If you pass a TextModel: calls model(ids, mask) and uses returned attention.
    If you pass a Fusion model: uses its internal self.text (TextModel) to get attention.
    Displays a bar plot for top_k tokens and returns a dict {token: weight}.
    """
    # Resolve TextModel
    text_model = None
    model = model_text_or_fusion
    if isinstance(model, TextModel):
        text_model = model
    else:
        # Fusion: requires .text submodule
        if hasattr(model, 'text') and isinstance(model.text, TextModel):
            text_model = model.text
        else:
            raise ValueError("Model must be TextModel or a fusion with .text submodule")

    ids, mask, tabv, imgv, yv = get_example(row_idx)
    was = _ensure_eval(text_model)
    with torch.no_grad():
        _, attn, _ = text_model(ids, mask)
    _restore_mode(text_model, was)

    attn_np = attn[0].detach().cpu().numpy()
    tok_ids = ids[0].detach().cpu().numpy()

    # Map ids -> tokens using 'itos' discovered in notebook
    tokens = [itos[i] if i < len(itos) else f"<{i}>" for i in tok_ids]
    # Ignore pads where mask == 0
    valid = mask[0].detach().cpu().numpy().astype(bool)
    tokens = [t for t,v in zip(tokens, valid) if v]
    vals   = [a for a,v in zip(attn_np, valid) if v]

    # Top-k tokens
    order = np.argsort(vals)[::-1][:top_k]
    top_tokens = [tokens[i] for i in order]
    top_vals   = [float(vals[i]) for i in order]
    attributions = dict(zip(top_tokens, top_vals))

    if show_plot:
        plt.figure(figsize=(10, 4))
        plt.bar(top_tokens, top_vals)
        plt.xticks(rotation=60, ha='right')
        plt.title("Text Attention Weights (Top tokens)")
        plt.tight_layout()
        plt.show()

    return attributions

# --- 2) Tabular feature attribution (Integrated Gradients or Grad*Input) ---
def explain_tabular(model_tab_or_fusion, row_idx: int, feature_names=None, n_steps=64, show_plot=True):
    """
    If TabularModel: attributes its prediction to input features.
    If Fusion: attributes w.r.t the TabularModel by hooking into its forward.
    Returns a pandas.Series with per-feature attribution.
    """
    # Resolve TabularModel and a forward that outputs scalar prediction using ONLY tab
    tab_model = None
    model = model_tab_or_fusion
    if isinstance(model, TabularModel):
        tab_model = model.to(DEVICE)
        def f_tab(x):
            out, _ = tab_model(x)
            return out
    else:
        if hasattr(model, 'tab') and isinstance(model.tab, TabularModel):
            tab_model = model.tab.to(DEVICE)
            def f_tab(x):
                out, _ = tab_model(x)
                return out
        else:
            raise ValueError("Model must be TabularModel or a fusion with .tab submodule")

    ids, mask, tabv, imgv, yv = get_example(row_idx)
    x = tabv.detach().clone().requires_grad_(True)

    if _HAS_CAPTUM:
        ig = IntegratedGradients(lambda t: f_tab(t))
        attr = ig.attribute(x, baselines=torch.zeros_like(x), n_steps=n_steps)
        scores = attr[0].detach().cpu().numpy()
    else:
        # simple grad*input fallback
        was = _ensure_eval(tab_model)
        yhat = f_tab(x)
        yhat.sum().backward()
        grads = x.grad.detach().cpu().numpy()
        scores = (grads * x.detach().cpu().numpy())[0]
        _restore_mode(tab_model, was)

    # Build series
    if feature_names is None:
        # Try to use tab_cols if available
        try:
            names = list(tab_cols)
        except Exception:
            names = [f"f{i}" for i in range(len(scores))]
    else:
        names = feature_names

    s = pd.Series(scores, index=names).sort_values(ascending=False)

    if show_plot:
        plt.figure(figsize=(8, 4))
        s.head(20).plot(kind='bar')
        plt.title("Tabular Feature Attribution (Top 20)")
        plt.tight_layout()
        plt.show()

    return s

# --- 3) Image embedding attribution (note: on CLIP/embed dims, not pixels) ---
def explain_image_embedding(model_img_or_fusion, row_idx: int, top_k=20, show_plot=True):
    """
    Attributes prediction to image embedding channels (since the notebook uses precomputed image_embeds).
    Returns indices and scores of top_k channels.
    """
    img_model = None
    model = model_img_or_fusion
    if isinstance(model, ImageModel):
        img_model = model.to(DEVICE)
        def f_img(x):
            out, _ = img_model(x)
            return out
    else:
        if hasattr(model, 'img') and isinstance(model.img, ImageModel):
            img_model = model.img.to(DEVICE)
            def f_img(x):
                out, _ = img_model(x)
                return out
        else:
            raise ValueError("Model must be ImageModel or a fusion with .img submodule")

    ids, mask, tabv, imgv, yv = get_example(row_idx)
    x = imgv.detach().clone().requires_grad_(True)

    if _HAS_CAPTUM:
        ig = IntegratedGradients(lambda t: f_img(t))
        attr = ig.attribute(x, baselines=torch.zeros_like(x), n_steps=64)
        scores = attr[0].detach().cpu().numpy()
    else:
        was = _ensure_eval(img_model)
        yhat = f_img(x)
        yhat.sum().backward()
        grads = x.grad.detach().cpu().numpy()
        scores = (grads * x.detach().cpu().numpy())[0]
        _restore_mode(img_model, was)

    # Get top-k channels
    order = np.argsort(np.abs(scores))[::-1][:top_k]
    top_idx = order.tolist()
    top_scores = scores[order].tolist()

    if show_plot:
        plt.figure(figsize=(8, 4))
        plt.bar([str(i) for i in top_idx], top_scores)
        plt.title("Image Embedding Channel Attribution (Top)")
        plt.tight_layout()
        plt.show()

    return list(zip(top_idx, top_scores))

# --- 4) Modality contribution via ablation (for Fusion models) ---
def modality_ablation(fusion_model, row_idx: int, zero_value="zeros"):
    """
    For Early/Late/Hybrid fusion: compute prediction with all modalities, then drop each modality by zeroing its input.
    zero_value: "zeros" or "mean" (mean of training set if accessible via tab_scaled.mean etc. Here we use zeros.)
    Returns a dict with baseline and per-modality dropped predictions.
    """
    if not isinstance(fusion_model, (EarlyFusion, LateFusion, HybridFusion)):
        raise ValueError("Pass a fusion model instance (EarlyFusion / LateFusion / HybridFusion).")

    ids, mask, tabv, imgv, yv = get_example(row_idx)
    fusion_model = fusion_model.to(DEVICE)
    was = _ensure_eval(fusion_model)

    with torch.no_grad():
        full_pred, extra = fusion_forward(fusion_model, ids, mask, tabv, imgv)

        # Helper to make ablated predictions
        def pred(ids_, mask_, tab_, img_):
            p, _ = fusion_forward(fusion_model, ids_, mask_, tab_, img_)
            return p

        # build zeroed
        if zero_value == "zeros":
            tab0 = torch.zeros_like(tabv)
            img0 = torch.zeros_like(imgv)
            ids0 = torch.zeros_like(ids)
            mask0 = torch.zeros_like(mask)
        else:
            tab0, img0, ids0, mask0 = tabv*0, imgv*0, ids*0, mask*0  # same as zeros

        # Drop each modality
        p_no_text = pred(ids0, mask0, tabv, imgv)
        p_no_tab  = pred(ids, mask, tab0, imgv)
        p_no_img  = pred(ids, mask, tabv, img0)

    _restore_mode(fusion_model, was)

    return {
        "y_true": float(yv.item()),
        "full": float(full_pred.item()),
        "no_text": float(p_no_text.item()),
        "no_tab": float(p_no_tab.item()),
        "no_img": float(p_no_img.item()),
        "delta_text": float(full_pred.item() - p_no_text.item()),
        "delta_tab": float(full_pred.item() - p_no_tab.item()),
        "delta_img": float(full_pred.item() - p_no_img.item()),
    }

def fusion_forward(model, ids, mask, tabv, imgv):
    """Unified forward for fusion models returning (pred, extras)"""
    if isinstance(model, EarlyFusion):
        yhat = model(ids, mask, tabv, imgv)
        # EarlyFusion may or may not return attn; normalize to (tensor, dict)
        if isinstance(yhat, tuple):
            return yhat[0], (yhat[1] if len(yhat)>1 else {})
        return yhat, {}
    elif isinstance(model, LateFusion):
        yhat, extra = model(ids, mask, tabv, imgv)
        return yhat, (extra if isinstance(extra, dict) else {})
    elif isinstance(model, HybridFusion):
        yhat = model(ids, mask, tabv, imgv)
        if isinstance(yhat, tuple):
            return yhat[0], (yhat[1] if len(yhat)>1 else {})
        return yhat, {}
    else:
        raise ValueError("Unsupported fusion model type.")

# --- 5) Global tabular permutation importance (validation set) ---
def permutation_importance_tabular(model_tab_or_fusion, loader, feature_names=None, repeats=5, max_batches=10):
    """
    Estimates global importance for tabular features via permutation on a validation DataLoader.
    Works for TabularModel or Fusion (perturbs only tabular inputs).
    Returns a pandas.Series sorted descending by importance (MAE increase).
    """
    # Resolve a forward function that uses full model but allows tab flip-in
    model = model_tab_or_fusion.to(DEVICE)
    if isinstance(model, TabularModel):
        def f(ids, mask, tabv, imgv):  # ignore other modalities
            out, _ = model(tabv)
            return out
    else:
        if not hasattr(model, "forward"):
            raise ValueError("Model must be a nn.Module with forward")
        def f(ids, mask, tabv, imgv):
            yhat = model(ids, mask, tabv, imgv)
            if isinstance(yhat, tuple): yhat = yhat[0]
            return yhat

    # Collect baseline predictions and y
    model.eval()
    all_y, all_pred = [], []
    batches = 0
    with torch.no_grad():
        for ids, mask, tabv, imgv, yb in loader:
            ids, mask = ids.to(DEVICE), mask.to(DEVICE)
            tabv, imgv, yb = tabv.to(DEVICE), imgv.to(DEVICE), yb.to(DEVICE)
            yhat = f(ids, mask, tabv, imgv)
            all_y.append(yb.detach().cpu().numpy())
            all_pred.append(yhat.detach().cpu().numpy())
            batches += 1
            if batches >= max_batches: break
    y_true = np.concatenate(all_y)
    base_pred = np.concatenate(all_pred)
    base_mae = np.mean(np.abs(base_pred - y_true))

    # Permute each feature
    F = tabv.shape[-1]
    if feature_names is None:
        try:
            names = list(tab_cols)
        except Exception:
            names = [f"f{i}" for i in range(F)]
    else:
        names = feature_names

    imp = np.zeros(F, dtype=np.float64)
    rng = np.random.default_rng(42)
    # We'll iterate over a fresh pass each feature
    for j in range(F):
        maes = []
        for r in range(repeats):
            batches = 0
            preds = []
            with torch.no_grad():
                for ids, mask, tabv, imgv, yb in loader:
                    ids, mask = ids.to(DEVICE), mask.to(DEVICE)
                    tabv, imgv = tabv.to(DEVICE), imgv.to(DEVICE)
                    # permute column j within batch
                    tabp = tabv.clone()
                    idx = torch.randperm(tabp.size(0), device=tabp.device)
                    tabp[:, j] = tabp[idx, j]
                    yhat = f(ids, mask, tabp, imgv)
                    preds.append(yhat.detach().cpu().numpy())
                    batches += 1
                    if batches >= max_batches: break
            preds = np.concatenate(preds)
            maes.append(np.mean(np.abs(preds - y_true[:len(preds)])))
        imp[j] = np.mean(maes) - base_mae

    s = pd.Series(imp, index=names).sort_values(ascending=False)
    return s

# --- Convenience: pretty print modality ablation ---
def plot_modality_ablation(result_dict, show_plot=True):
    keys = ["delta_text", "delta_tab", "delta_img"]
    if show_plot:
        plt.figure(figsize=(4,3))
        vals = [result_dict[k] for k in keys]
        plt.bar(["Text","Tabular","Image"], vals)
        plt.title("Contribution (Î” prediction when removed)")
        plt.tight_layout()
        plt.show()
    return {k: result_dict[k] for k in keys}

"""**function for explaination of the row using this model **"""

def run_explainability(model, row_idx, df, tab_cols, val_loader):
    row = df.iloc[row_idx]

    print("\n=== Row Info ===")
    print(row[["video_id", "title", "tags", "view_count", "likes", "comment_count"]])

    print("\n--- Text Attention ---")
    visualize_text_attention(model, row_idx)

    print("\n--- Tabular Feature Attribution ---")
    tab_attr = explain_tabular(model, row_idx, feature_names=list(tab_cols))
    print(tab_attr.head(10))

    print("\n--- Image Embedding Attribution ---")
    img_attr = explain_image_embedding(model, row_idx, top_k=15)
    print(img_attr[:10])

    print("\n--- Modality Ablation ---")
    abl = modality_ablation(model, row_idx)
    print(abl)
    plot_modality_ablation(abl)

    print("\n--- Global Permutation Importance ---")
    global_imp = permutation_importance_tabular(
        model, val_loader, feature_names=list(tab_cols), repeats=3, max_batches=10
    )
    print(global_imp.head(15))

    return {
        "tab_attr": tab_attr,
        "img_attr": img_attr,
        "abl": abl,
        "global_imp": global_imp,
    }

"""choose the row number -- examples for usage"""

results = run_explainability(early_fusion, row_idx=3, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=300, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=50, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=100, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=47, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=633, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=1000, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=945, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=865, df=df, tab_cols=tab_cols, val_loader=val_loader)

results = run_explainability(early_fusion, row_idx=790, df=df, tab_cols=tab_cols, val_loader=val_loader)

"""1. visualize_text_attention(early_fusion, row)

What you see: A bar plot of words (tokens) in the video title/description with their attention weights.
Meaning:

High bar = the model focused heavily on that word when predicting engagement.

Example: if words like â€œgiveawayâ€ or â€œofficial trailerâ€ have high weights, it means the model learned that those terms strongly influence engagement rate.

2. explain_tabular(early_fusion, row, feature_names=list(tab_cols))

What you see: A ranked list (or DataFrame) of tabular features with attribution scores (positive or negative).
Meaning:

Positive score = this feature increased the predicted engagement.

Negative score = this feature decreased the prediction.

Example:

like_count â†’ strong positive attribution (drives engagement up).

view_count â†’ negative attribution (large denominator in engagement rate).

3. explain_image_embedding(early_fusion, row, top_k=15)

What you see: Top-k embedding channels (from the CLIP-like image embeddings) that most influenced the prediction.
Meaning:

Since your model doesnâ€™t use raw pixels but image embeddings, the â€œchannelsâ€ represent abstract visual concepts (e.g., color patterns, objects, faces).

High attribution on certain channels = the thumbnailâ€™s visual style strongly impacted engagement prediction.

4. modality_ablation(early_fusion, row) & plot_modality_ablation(abl)

What you see: A bar chart of prediction values when each modality (text/tabular/image) is removed one at a time.
Meaning:

Big drop when removing text â†’ text was critical for this prediction.

Small/no drop when removing image â†’ image had little effect for this video.

This tells you which modality the model relied on most for that sample.

5. permutation_importance_tabular(early_fusion, val_loader, ...)

What you see: Global ranking of tabular features across the validation set.
Meaning:

It tests what happens if you randomly shuffle each feature column â†’ if performance drops a lot, that feature is important globally.

Example:

comment_count and likes might show up as the most globally important.

categoryId might have lower importance.

 In short:

Text attention = per-word importance for that video.

Tabular attribution = per-feature importance for that video.

Image attribution = which parts of the thumbnail embedding mattered.

Modality ablation = which modality (text/tab/image) mattered most.

Global importance = which tabular features matter overall (all videos).

##  ) Inference: Paste a YouTube URL
"""

